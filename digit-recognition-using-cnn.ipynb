{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/digit-recognizer/train.csv\n",
      "/kaggle/input/digit-recognizer/test.csv\n",
      "/kaggle/input/digit-recognizer/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n",
    "test_images=pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n",
    "train_images=train_data.iloc[:,1:]\n",
    "train_labels=train_data.iloc[:,0]\n",
    "train_images=train_images.to_numpy()\n",
    "train_labels=train_labels.to_numpy()\n",
    "test_images=test_images.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape data\n",
    "train_images=train_images.reshape(train_images.shape[0],28,28,1)\n",
    "train_labels=train_labels.reshape(train_labels.shape[0],1)\n",
    "test_images=test_images.reshape(test_images.shape[0],28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(imgs): # should work for both a single image and multiple images\n",
    "    sample_img = imgs if len(imgs.shape) == 2 else imgs[0]\n",
    "    assert sample_img.shape in [(28, 28, 1), (28, 28)], sample_img.shape # make sure images are 28x28 and single-channel (grayscale)\n",
    "    return imgs / 255.0\n",
    "\n",
    "train_images = preprocess_images(train_images)\n",
    "test_images = preprocess_images(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/matplotlib/text.py:1191: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,2))\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i].reshape(28, 28), cmap=plt.cm.binary)\n",
    "    plt.xlabel(train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "# 32 convolution filters used each of size 3x3\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# 64 convolution filters used each of size 3x3\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# choose the best features via pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# randomly turn neurons on and off to improve convergence\n",
    "model.add(Dropout(0.25))\n",
    "# flatten since too many dimensions, we only want a classification output\n",
    "model.add(Flatten())\n",
    "# fully connected to get all relevant data\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# one more dropout\n",
    "model.add(Dropout(0.5))\n",
    "# output a softmax to squash the matrix into output probabilities\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/5\n",
      "33600/33600 [==============================] - 52s 2ms/sample - loss: 1.5867 - accuracy: 0.8785 - val_loss: 1.5011 - val_accuracy: 0.9610\n",
      "Epoch 2/5\n",
      "33600/33600 [==============================] - 51s 2ms/sample - loss: 1.5167 - accuracy: 0.9456 - val_loss: 1.4881 - val_accuracy: 0.9729\n",
      "Epoch 3/5\n",
      "33600/33600 [==============================] - 51s 2ms/sample - loss: 1.5056 - accuracy: 0.9561 - val_loss: 1.4858 - val_accuracy: 0.9749\n",
      "Epoch 4/5\n",
      "33600/33600 [==============================] - 50s 1ms/sample - loss: 1.5005 - accuracy: 0.9613 - val_loss: 1.4801 - val_accuracy: 0.9818\n",
      "Epoch 5/5\n",
      "33600/33600 [==============================] - 52s 2ms/sample - loss: 1.4968 - accuracy: 0.9650 - val_loss: 1.4807 - val_accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb7bbfed0f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([model, \n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = probability_model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_file(submission):\n",
    "    submission=np.argmax(predictions,axis=1)\n",
    "    submission=pd.DataFrame(submission)\n",
    "    submission.index = np.arange(1, len(submission)+1)\n",
    "    submission= submission.rename_axis('ImageId').reset_index()\n",
    "    submission=submission.rename(columns={0:'Label'})\n",
    "    submission.index = np.arange(1, len(submission)+1)\n",
    "    return submission\n",
    "submission=submission_file(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28000</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "1            1      2\n",
       "2            2      0\n",
       "3            3      9\n",
       "4            4      9\n",
       "5            5      3\n",
       "...        ...    ...\n",
       "27996    27996      9\n",
       "27997    27997      7\n",
       "27998    27998      3\n",
       "27999    27999      9\n",
       "28000    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
